// utils/localAI.js
import fetch from "node-fetch";

const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || "http://localhost:11434";
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || "llama2"; // Ø£Ùˆ mistral, mixtral

/**
 * Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ollama ÙŠØ¹Ù…Ù„
 */
export async function checkOllamaHealth() {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 5000);
    
    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {
      method: "GET",
      signal: controller.signal,
    });
    
    clearTimeout(timeoutId);

    if (response.ok) {
      const data = await response.json();
      const models = data.models || [];
      const hasModel = models.some((m) => m.name.includes(OLLAMA_MODEL));
      
      return {
        available: true,
        models: models.map((m) => m.name),
        targetModel: OLLAMA_MODEL,
        hasTargetModel: hasModel,
      };
    }
    
    return { 
      available: false, 
      error: `Ollama server responded with status ${response.status}` 
    };
  } catch (error) {
    if (error.name === 'AbortError') {
      return {
        available: false,
        error: "Connection timeout - Ollama may not be running",
      };
    }
    
    if (error.code === 'ECONNREFUSED' || error.message?.includes('ECONNREFUSED')) {
      return {
        available: false,
        error: "Ollama is not running. Please start it with: ollama serve",
      };
    }
    
    return {
      available: false,
      error: error.message || "Cannot connect to Ollama server",
    };
  }
}

/**
 * Ø¥Ø±Ø³Ø§Ù„ Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ollama
 */
export async function askOllama(prompt, options = {}) {
  const {
    model = OLLAMA_MODEL,
    temperature = 0.7,
    max_tokens = 2000,
    stream = false,
  } = options;

  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 120000); // 2 minutes
    
    const response = await fetch(`${OLLAMA_BASE_URL}/api/generate`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: model,
        prompt: prompt,
        stream: stream,
        options: {
          temperature: Math.min(temperature || 0.3, 0.5), // Cap at 0.5 for faster, focused responses
          num_predict: Math.min(max_tokens || 800, 1000), // Cap at 1000 tokens for speed
          top_p: 0.8,
          top_k: 30,
        },
      }),
      signal: controller.signal,
    });
    
    clearTimeout(timeoutId);

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(
        `Ollama API error (${response.status}): ${errorText}`
      );
    }

    const data = await response.json();
    return data.response || "";
  } catch (error) {
    if (error.name === 'AbortError') {
      throw new Error("Request timeout - The model may be too large or slow");
    }
    
    if (error.code === 'ECONNREFUSED' || error.message?.includes('ECONNREFUSED')) {
      throw new Error("Ollama is not running. Please start it with: ollama serve");
    }
    
    console.error("âŒ Ollama API Error:", error);
    throw error;
  }
}

/**
 * Ø¥Ø±Ø³Ø§Ù„ Ù…Ø­Ø§Ø¯Ø«Ø© (Chat) Ø¥Ù„Ù‰ Ollama
 * Ollama ÙŠØ¯Ø¹Ù… Chat API Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ù…Ù† Generate
 */
export async function chatWithOllama(messages, options = {}) {
  const {
    model = OLLAMA_MODEL,
    temperature = 0.7,
    max_tokens = 2000,
  } = options;

  try {
    // ØªØ­ÙˆÙŠÙ„ messages Ø¥Ù„Ù‰ format Ollama
    const formattedMessages = messages.map((msg) => ({
      role: msg.role === "system" ? "system" : msg.role,
      content: msg.content,
    }));

    // âœ… ØªØ¹Ø±ÙŠÙ controller Ù„Ù„Ù€ timeout (45 Ø«Ø§Ù†ÙŠØ© - Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø³Ø±Ø¹Ø©)
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 45000); // 45 seconds

    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: model,
        messages: formattedMessages,
        stream: false,
        options: {
          temperature: Math.min(temperature || 0.3, 0.5), // Cap at 0.5 for faster, focused responses
          num_predict: Math.min(max_tokens || 800, 1000), // Cap at 1000 tokens for speed
          top_p: 0.8,
          top_k: 30,
        },
      }),
      signal: controller.signal,
    });
    
    clearTimeout(timeoutId);

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(
        `Ollama API error (${response.status}): ${errorText}`
      );
    }

    const data = await response.json();
    return data.message?.content || data.response || "";
  } catch (error) {
    if (error.name === 'AbortError') {
      throw new Error("Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ - Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‚Ø¯ ÙŠÙƒÙˆÙ† ÙƒØ¨ÙŠØ±Ø§Ù‹ Ø£Ùˆ Ø¨Ø·ÙŠØ¦Ø§Ù‹. Ø¬Ø±Ø¨ Ù…ÙˆØ¯ÙŠÙ„ Ø£ØµØºØ± Ù…Ø«Ù„ llama2");
    }
    
    if (error.code === 'ECONNREFUSED' || error.message?.includes('ECONNREFUSED')) {
      throw new Error("Ollama is not running. Please start it with: ollama serve");
    }
    
    console.error("âŒ Ollama Chat API Error:", error);
    throw error;
  }
}

/**
 * ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
 */
export async function pullModel(modelName = OLLAMA_MODEL) {
  try {
    console.log(`ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ${modelName}...`);
    
    const response = await fetch(`${OLLAMA_BASE_URL}/api/pull`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        name: modelName,
        stream: false,
      }),
      timeout: 600000, // 10 minutes for model download
    });

    if (!response.ok) {
      throw new Error(`Failed to pull model: ${response.statusText}`);
    }

    const data = await response.json();
    console.log(`âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ${modelName} Ø¨Ù†Ø¬Ø§Ø­`);
    return data;
  } catch (error) {
    console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ${modelName}:`, error);
    throw error;
  }
}
